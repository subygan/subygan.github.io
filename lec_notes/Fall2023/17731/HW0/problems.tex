

\noindent
\begin{enumerate}[leftmargin=\parindent, itemsep=3ex]

    \item{\bf Expectation and Variance}

    The notions of \textit{expected value} and \textit{variance} are central in the study of probability. Expected value, roughly, gives a sense of the ''center" of a probability distribution. For a discrete random variable $X$ (that is, a random variable which takes on at most countably many values), expected value is defined as $\E[X] = \sum_{x \in \Omega}x\P(X = x)$, where $\Omega \subset \R$ is some at most countable set. For a continuous random variable $Y$ with density $f(x)$, the expectation is given by $\E[Y] = \int_{\R}xf(x)dx$.

    The variance of a random variable $X$ (either continuous or discrete) is given by $Var[X] = \E\left(X - \E[X]\right)^2$. Roughly, the variance tells us how spread out a distribution is with respect to its center. Please try to answer the following questions about expected value and variance.

    \begin{enumerate}
        \item Let $X$ be a random variable. Show that the variance of $X$ can be expressed as $Var[X] = \E[X^2] - \E[X]^2$.
        \item Suppose $X$ and $Y$ are \textit{independent random variables}, i.e., for any events $A, B \subset \R$, we have $$\P(X \in A, Y\in B) = \P(X \in A)\P(Y \in B)$$ (In terms of densities, if $f(x, y)$ is the joint density of $X$ and $Y$, independence implies $f(x, y) = f_X(x)f_Y(y)$, where $f_X$ and $f_Y$ are the marginal densities of $X$ and $Y$). Show that $\E[XY] = \E[X]\E[Y]$.
        \item Suppose $X, Y$ are independent random variables. Show that $Var[aX + bY] = a^2Var[X] + b^2Var[Y]$. (The previous part should be useful).
        \item Let $X$ be a random variable. Which constant $B \in \R$ minimizes $\E\left[\left(X - B\right)^2\right]$? What does this say about $\E[X]$ as a measure of the center of the distribution of $X$?
        \item Lastly, suppose $X_1, \dots, X_n$ are independent random variables with mean $\mu$ and variance $\sigma^2$. Define the \textit{sample average} to be $\overline{X} = \frac{1}{n}(X_1 + \cdots + X_n)$. What is $\E[\overline{X}]$? What about $Var[\overline{X}]$?
        \item * Suppose $X$ is a non-negative random variable. Show we have the equivalence $\E[X] = \int_0^\infty \P(X > t)dt$.
        \item * Now, which constant $B \in \R$ minimizes $\E\left[|X - B|\right]$?
    \end{enumerate}

    \item{\bf Key Distributions}

    There are many important distributions which play a central role in fields such as differential privacy, statistics, and machine learning. While you may have heard of the following distributions, it is always good to brush up on key properties.

    \begin{enumerate}
        \item Suppose we toss one biased coin with probability of heads $p \in (0, 1)$. Let $X = 1$ if the coin turns up heads, $X = 0$ if the coin lands tails. What is $\E[X]$? What about $Var[X]$? $X$ is said to be a Bernoulli random variable with probability of success $p$. We write $X \sim Ber(p)$.
        \item Now, suppose we toss $n$ independent coins, each of which has probability $p$ of turning up heads. Let $N$ denote the number of heads out of the $n$ tosses. What is $\P(N = k)$, for $k = 0, 1, \dots, n$? What is $\E[N]$? What about $Var[N]$? (Hint: these last two points can be solved easily with the previous question). Such a random variable is said to be Binomially distributed with $n$ trials and probability of success $p$. We write $X \sim Bin(n, p)$ correspondingly.
        \item Instead of tossing a fixed number of coins, you now toss independent coins until you see the first heads, where $p$ is still the probability that any given coin lands heads up. Let $N$ represent this number of tosses. What is $\P(N = n)$ for $n \in \N$? Likewise, what is $\E[N]$? $Var[N]$? The random variable with said distribution is said to be \textit{Geometrically distributed with parameter $p$}. We write this as $N \sim Geo(p)$.
        \item * Now, we talk about some continuous distributions. $X$ is said to be \textit{exponentially distributed with parameter $\lambda$} if $X$ has density $f(x) = \lambda e^{-\lambda x}$ for all non-negative $x$. What is $\E[X]$? What about $Var[X]$? Show, using \textit{Bayes rule}, that $\P(X > t + s| X > s) = \P(X > t)$, for any $s, t \geq 0$. In words, what does this equality say?
        \item * We say $X$ is \textit{normally distributed with mean $\mu$ and variance $\sigma^2$}, denoted $X \sim N(\mu, \sigma^2)$, if $X$ has density $f(x) = \frac{1}{\sqrt{2\pi}\sigma}e^{\frac{-(x - \mu)^2}{2\sigma^2}}$. Show that if $X$ and $Y$ are independent random variables with $X \sim N(\mu_1, \sigma_1^2)$ and $Y \sim N(\mu_2, \sigma_2^2)$, then $X + Y \sim N(\mu_1 + \mu_2, \sigma_1^2 + \sigma_2^2)$ (Hint: you may need to look up something called \textit{convolution}).
    \end{enumerate}


    \item{\bf Basic Probability Computations}

    The first three sections asked about theoretical aspects of probability theory, but probability is also useful for doing computations! Try your hand at a few of the following problems!

    \begin{enumerate}
        \item Suppose you roll 3 dice consecutively. What is the probability that the values shown by the dice are in \textit{strictly} increasing order?
        \item Suppose there is a 50 percent chance that it rains on any given day. Moreover, assume that whether or not it rains on any given day is independent of whether or not it will rain on any other given day. What is the probability that it rains at least once in a 7 day week.
        \item Suppose there are 25 people in a room, and each person has their birthday drawn uniformly at random from the 365 days of a year (sorry leap years!). What is the expected number of people that share a birthday with at least one other person?
    \end{enumerate}
\end{enumerate}